{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 Regression Models\n",
    "-----------------\n",
    "\n",
    "## Authors\n",
    "\n",
    "* Filipe Ferreira, Student, up201706086@fe.up.pt\n",
    "* Mark Meehan, Student,     up201704581@fe.up.pt\n",
    "* Sofia Lajes, Student,     up201704066@fe.up.pt\n",
    "\n",
    "### Afiliattions\n",
    "\n",
    "[![feup_logo](images/logo_cores_oficiais.jpg)](http://www.fe.up.pt)\n",
    "\n",
    "Faculdade de Engenharia, Universidade do Porto Rua Dr. Roberto Frias, 4200-465 Porto, Portugal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "The outbreak of the novel Coronavirus disease (COVID-19) has been one worse outbreaks in human history, leaving thousands unemployed and economies to a halt. Each country has faced the pandemic in many different ways, some quarantining and closing the borders earlier than others. The purpose of this paper is to explore how we can train Machine Learning, more specifically Supervised Learning, models to predict the number of cases, deaths and recoveries in each country.\n",
    "\n",
    "**Keywords :** Coronavirus, outbreak, COVID-19, Data Science, Machine Learning, Regression, Data Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Given the current world pandemic health risk, it seemed relevant and of significant importance to study the current outbreak and develop a better understanding of the challenges that lay ahead. \n",
    "\n",
    "Therefore, we chose to study Covid-19 and develop models for it's regression in the near future, in many countries, but especially the ones that were more severely affected by the pandemic outbreak. For predicting the number of confirmed cases, deaths and recovered patients of Covid-19 in each country, we developed a set of models and analysed it's score and accuracy for the current data. These models, in specific, were a Neural Network,  K-Nearest-Neighbour, and Support Vector Machine.  \n",
    "\n",
    "The same input data was used for all of these models - Days since January 2020, Population Density, Urban Population, and Total Population Percentage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "%matplotlib inline\n",
    "%reload_ext watermark\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "from IPython.display import display\n",
    "import sklearn as sk\n",
    "import sklearn.neural_network as sknn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.svm import SVR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "In this section we load both datasets we use, COVID-19 dataset and countries population dataset , clean the data and add some new columns.  \n",
    "\n",
    "The datasets we use are:\n",
    "\n",
    "* https://www.kaggle.com/imdevskp/corona-virus-report for the COVID-19 dataset\n",
    "\n",
    "* https://www.kaggle.com/tanuprabhu/population-by-country-2020 for population information per country\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COVID-19 Dataset\n",
    "\n",
    "This dataset is updated daily in the kraggle repository, to check how it was gathered and clean check the link above or in the references. This notebook was made locally and comes with a script to download the datasets from kraggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading datasets\n",
    "\n",
    "full_table = pd.read_csv('datasets/covid_19_clean_complete.csv', \n",
    "                          na_values=['NaN'],\n",
    "                          parse_dates=['Date'])\n",
    "\n",
    "# Adding Active cases column\n",
    "full_table['Active'] = (full_table['Confirmed'] - full_table['Deaths'] - full_table['Recovered']).apply(lambda x: x if x >= 0 else 0)\n",
    "\n",
    "# filling missing values\n",
    "full_table[['Province/State']] = full_table[['Province/State']].fillna('')\n",
    "full_table[['Confirmed','Deaths','Recovered','Active']] = full_table[['Confirmed','Deaths','Recovered','Active']].fillna(0)\n",
    "\n",
    "full_table.sample(6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_table = pd.read_csv('datasets/population_by_country_2020.csv',\n",
    "                        na_values=['N.A.'])\n",
    "\n",
    "\n",
    "# Selecting only the Country and Population columns\n",
    "pop_table = pop_table.iloc[:,[0,1,4,9]]\n",
    "\n",
    "\n",
    "\n",
    "# Renaming columns\n",
    "pop_table.columns = ['Country/Region', 'Population', 'Population Density (P/Km²)','Urban Population %']\n",
    "\n",
    "# Most of the entries with urban population as NaN in the population dataset that we are going to use have 100% as of 2020\n",
    "pop_table[['Urban Population %']] = pop_table[['Urban Population %']].fillna('100 %')\n",
    "pop_table['Urban Population %'] = pop_table['Urban Population %'].map(lambda x: int(x.split(' ')[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_table.info()\n",
    "pop_table.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing ship data\n",
    "\n",
    "The dataset also includes data from the various ships that had COVID19 outbreaks. Since we only need the information per country we removed it from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ship rows\n",
    "ship_rows = full_table['Province/State'].str.contains('Grand Princess') | full_table['Province/State'].str.contains('Diamond Princess') | full_table['Country/Region'].str.contains('Diamond Princess') | full_table['Country/Region'].str.contains('MS Zaandam')\n",
    "\n",
    "# ship\n",
    "ship = full_table[ship_rows]\n",
    "\n",
    "# dropping ship rows \n",
    "full_table = full_table[~(ship_rows)]\n",
    "\n",
    "ship.sample(6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing country names\n",
    "\n",
    "\n",
    "### Fixing mismatched names between datasets\n",
    "\n",
    "Here we manually set the names so that the join between datasets works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_name_only = {\n",
    "    'Sao Tome & Principe': 'Sao Tome and Principe',\n",
    "    \"Côte d'Ivoire\": \"Cote d'Ivoire\",\n",
    "    \"United States\": \"US\",\n",
    "    \"Czech Republic (Czechia)\": 'Czechia',\n",
    "    'Myanmar': 'Burma',\n",
    "    'Taiwan': 'Taiwan*',\n",
    "    'Saint Kitts & Nevis': 'Saint Kitts and Nevis',\n",
    "    'Macao' : 'Macau'\n",
    "}\n",
    "\n",
    "for original,new in fix_name_only.items():\n",
    "    full_table.loc[full_table['Country/Region'] == new, 'Country/Region'] = original\n",
    "    full_table.loc[full_table['Province/State'] == new, 'Province/State'] = original\n",
    "\n",
    "missing_countries = set(full_table['Country/Region']).difference(set(pop_table['Country/Region']))\n",
    "\n",
    "# # print(sorted(pop_table['Country/Region'].unique()))\n",
    "# if len(missing_countries) != 0:\n",
    "#     print(missing_countries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing Country/Region with Province/State\n",
    "\n",
    "The population dataset has entries for autonomous regions, for example Greenland. Here we rewrite the Country/Region column with the Province/State name so we can easily join the population dataset. For example, Greenland exists in the population dataset so what we do is replace Denmark (the Country column of Greenland) with Greenland."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "province_set = set(full_table['Province/State']).intersection(set(pop_table['Country/Region']))\n",
    "\n",
    "no_data = set(['Saint Vincent and the Grenadines','Kosovo','Congo','West Bank and Gaza'])\n",
    "\n",
    "for province in province_set:\n",
    "    if province in no_data:\n",
    "        continue\n",
    "    full_table.loc[ full_table['Province/State'] == province,'Country/Region'] = province \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "full_table.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping data\n",
    "\n",
    "Here we are grouping data by Date and Country so we can add population and cases per million afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group by Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_grouped = full_table.groupby(['Country/Region','Lat','Long','Date'])['Confirmed','Deaths','Recovered','Active'].sum().reset_index()\n",
    "full_grouped_nolat = full_table.groupby(['Country/Region','Date'])['Confirmed','Deaths','Recovered','Active'].sum().reset_index()\n",
    "\n",
    "full_grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding population\n",
    "In this section, we merge both datasets by Country/Region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_grouped = pd.merge(full_grouped,pop_table,on=['Country/Region'])\n",
    "full_grouped_nolat = pd.merge(full_grouped_nolat,pop_table,on=['Country/Region'])\n",
    "\n",
    "full_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating new cases per day\n",
    "\n",
    "To calculate the number of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe with latitude and longitude\n",
    "temp = full_grouped.groupby(['Country/Region', 'Date'])['Confirmed', 'Deaths', 'Recovered']\n",
    "temp = temp.sum().diff().reset_index()\n",
    "\n",
    "mask = temp['Country/Region'] != temp['Country/Region'].shift(1)\n",
    "\n",
    "temp.loc[mask, 'Confirmed'] = np.nan\n",
    "temp.loc[mask, 'Deaths'] = np.nan\n",
    "temp.loc[mask, 'Recovered'] = np.nan\n",
    "\n",
    "\n",
    "temp.columns = ['Country/Region', 'Date','New cases', 'New deaths', 'New recovered']\n",
    "\n",
    "\n",
    "full_grouped = pd.merge(full_grouped,temp, on=['Country/Region', 'Date'])\n",
    "\n",
    "full_grouped = full_grouped.fillna(0)\n",
    "\n",
    "full_grouped[['New cases','New deaths','New recovered']] = full_grouped[['New cases','New deaths','New recovered']].astype('int64')\n",
    "\n",
    "#############################################################################################################################\n",
    "# # Dataset with no lat and long\n",
    "\n",
    "temp = full_grouped_nolat.groupby(['Country/Region', 'Date' ])['Confirmed', 'Deaths', 'Recovered']\n",
    "temp = temp.sum().diff().reset_index()\n",
    "\n",
    "mask = temp['Country/Region'] != temp['Country/Region'].shift(1)\n",
    "\n",
    "temp.loc[mask, 'Confirmed'] = np.nan\n",
    "temp.loc[mask, 'Deaths'] = np.nan\n",
    "temp.loc[mask, 'Recovered'] = np.nan\n",
    "\n",
    "temp.columns = ['Country/Region', 'Date', 'New cases', 'New deaths', 'New recovered']\n",
    "\n",
    "full_grouped_nolat = pd.merge(full_grouped_nolat,temp, on=['Country/Region', 'Date'])\n",
    "\n",
    "full_grouped_nolat = full_grouped_nolat.fillna(0)\n",
    "\n",
    "full_grouped_nolat[['New cases','New deaths','New recovered']] = full_grouped_nolat[['New cases','New deaths','New recovered']].astype('int64')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### World Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_data = full_grouped.groupby(['Date'])['Confirmed','Deaths','Recovered','Active','Population'].sum().reset_index()\n",
    "world_data.loc[world_data['Date'] == world_data['Date'].max()]\n",
    "world_data.head()\n",
    "world_pop_total = world_data['Population'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check information on types and null values\n",
    "full_grouped.info()\n",
    "full_grouped.loc[full_grouped['Urban Population %'].isnull()]['Country/Region'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_grouped.sample(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Cases per Million of People\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_permillion(df):\n",
    "    df['Confirmed per million'] = round((df['Confirmed'] / df['Population']) * 1000000)\n",
    "    df['Deaths per million']    = round((df['Deaths'] / df['Population']) * 1000000)\n",
    "    df['Recovered per million'] = round((df['Recovered'] / df['Population']) * 1000000)\n",
    "    df['Active per million']    = round((df['Active'] / df['Population']) * 1000000)\n",
    "    return df\n",
    "\n",
    "def calc_permillion_world():\n",
    "    world_data['Confirmed per million'] = round((world_data['Confirmed'] / world_data['Population']) * 1000000)\n",
    "    world_data['Deaths per million']    = round((world_data['Deaths'] / world_data['Population']) * 1000000)\n",
    "    world_data['Recovered per million'] = round((world_data['Recovered'] / world_data['Population']) * 1000000)\n",
    "    world_data['Active per million']    = round((world_data['Active'] / world_data['Population']) * 1000000)\n",
    "\n",
    "\n",
    "\n",
    "per_million = calc_permillion(full_grouped)\n",
    "per_million_nolat = calc_permillion(full_grouped_nolat)\n",
    "\n",
    "per_million.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Truncation\n",
    "\n",
    "Since the COVID-19 dataset only has data from  22nd of January of 2020 onwards, we will define Date from here moving foward as days since the 22nd of January of 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_million['Date'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function does that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daysSinceJan(d):\n",
    "    return d.toordinal() - datetime(2020,1,20).toordinal()\n",
    "\n",
    "def revertdaysSince(d):\n",
    "    return datetime(2020,1,20) + timedelta(days=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_million_nolat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_million['Days Since Jan'] = per_million['Date'].map(daysSinceJan)\n",
    "\n",
    "per_million_nolat['Days Since Jan'] = per_million['Date'].map(daysSinceJan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_million =  per_million.sort_values(['Date','Country/Region'],ascending=[True, True])\n",
    "per_million.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_million_nolat.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding population percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_million['Pop %'] = per_million['Population'].apply(lambda x: x/world_pop_total * 100)\n",
    "per_million_nolat['Pop %'] = per_million_nolat['Population'].apply(lambda x: x/world_pop_total * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_million_nolat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeChinaData(df):\n",
    "    df = df[full_grouped['Country/Region'] != 'China']\n",
    "\n",
    "removeChinaData(full_grouped)\n",
    "removeChinaData(full_grouped_nolat)\n",
    "removeChinaData(per_million_nolat)\n",
    "removeChinaData(per_million)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_million.loc[per_million['Country/Region'] == 'Portugal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "We decided to train three different models provided by the scikit-learn API: MLPRegressor (Multi layer Perceptron), KNeighborsRegressor and a SVR (Support Vector Machine). Each of the following sections explain the input and output and each algorithm and how they are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs and outputs\n",
    "\n",
    "This function will return a pair of inputs and outputs given the country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def print_graph(world_viz,things = ['Confirmed','Deaths','Recovered'],label='C/D/R',x = 'Date',title='Title',limit_x=None):\n",
    "    sb.set()\n",
    "\n",
    "    dd = world_viz.melt([x],var_name=label, value_name='Cases',value_vars=things)\n",
    "\n",
    "\n",
    "    chart = sb.relplot(x=x,y='Cases',hue=label,data=dd,kind='line')\n",
    "\n",
    "    chart.set(title=title)\n",
    "    chart.fig.autofmt_xdate()\n",
    "    if limit_x is not None:\n",
    "        plt.axvline(limit_x,0,1,linewidth=1, color='r',linestyle='--')\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def getPredInp(country):\n",
    "    return [per_million.loc[per_million['Country/Region'] == country]['Population Density (P/Km²)'].max(),\n",
    "    per_million.loc[per_million['Country/Region'] == country]['Urban Population %'].max(),\n",
    "    per_million.loc[full_grouped['Country/Region'] == country]['Pop %'].max()]\n",
    "\n",
    "# per_million.loc[per_million['Country/Region'] == 'Portugal'].groupby(['Lat','Long']).size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def print_predict(model,country,minprev=0,offset=10,maxprev=daysSinceJan(per_million_nolat['Date'].max()),print_real=True,title=\"Future Prediction\"):\n",
    "\n",
    "    \n",
    "    country_pop = per_million_nolat.loc[per_million_nolat['Country/Region'] == country]['Population'].max()\n",
    "    ip = []\n",
    "\n",
    "    for dat in range(minprev,maxprev+offset+1): # Predict from 0 to 155 days from January 1st 2020\n",
    "        ip.append([dat, *getPredInp(country)]) # Hard Coded Pop Density, Urban Pop %, Latitude and Longitude\n",
    "\n",
    "    out = model.predict(ip)\n",
    "\n",
    "    nl = []\n",
    "\n",
    "    for i,o in zip(ip,out):\n",
    "        nl.append([*i,*o])\n",
    "\n",
    "    futurepredict = pd.DataFrame(nl,columns=['Date','Population Density (P/Km²)','Urban Population %',\"Pop %\",'Confirmed','Deaths','Recovered'])\n",
    "\n",
    "    futurepredict['Confirmed'] = futurepredict['Confirmed'].map(lambda x: round((x/1000000) * country_pop))\n",
    "    futurepredict['Recovered'] = futurepredict['Recovered'].map(lambda x: round((x/1000000) * country_pop))\n",
    "    futurepredict['Deaths'] = futurepredict['Deaths'].map(lambda x: round((x/1000000) * country_pop) )\n",
    "\n",
    "    futurepredict['Date'] = futurepredict['Date'].map(revertdaysSince) \n",
    "\n",
    "    if print_real:\n",
    "        print_graph(per_million_nolat.loc[per_million_nolat['Country/Region'] == country],['Confirmed','Deaths','Recovered'], title='Real Data')\n",
    "    print_graph(futurepredict,['Confirmed','Deaths','Recovered'],x='Date', title=title,limit_x=revertdaysSince(maxprev))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLPRegressor\n",
    "\n",
    "The MLPRegressor stands for Multi Layer Perceptron. The MLP is very sensitive to feature scaling, so we tried to normalize the input data as much as we could, for example we didn't use the Population of the country, we used the percentage of the population relative to the world's population (more specifically the sum of all different countries population in the dataset). The activation function we found that is best for this dataset is the rectifier linear unit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_by_country_train(\n",
    "    country,\n",
    "    input=['Days Since Jan','Population Density (P/Km²)','Urban Population %','Pop %'],\n",
    "    out= ['Confirmed per million','Deaths per million','Recovered per million'],\n",
    "    hidden_layer_sizes=(100,100,100,60,60,60)\n",
    "):\n",
    "    df = per_million_nolat.loc[ per_million_nolat['Country/Region'] == country]\n",
    "    X = df[input].values\n",
    "    Y = df[out].values\n",
    "    nn = sknn.MLPRegressor(\n",
    "        hidden_layer_sizes=hidden_layer_sizes,\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=0.00001,\n",
    "        batch_size='auto',\n",
    "        max_iter=2000,\n",
    "        n_iter_no_change=500)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.30,shuffle=True)\n",
    "    nn.fit(X_train,y_train) # Train the model\n",
    "    y_pred = nn.predict(X_test)\n",
    "\n",
    "    nnr2 = nn.score(X_test,y_test) # Calculate R² for the model\n",
    "    nnmae = mean_absolute_error(y_test,y_pred) # Mean Absolute Error\n",
    "    nnmse = mean_squared_error(y_test,y_pred)\n",
    "\n",
    "    print(\"R2:\",nnr2)\n",
    "    print(\"MAE:\",nnmae)\n",
    "    print(\"MSE:\",nnmse)\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Mexico as an example here but you can change it to use other countries. Here we can see how the number of nodes per layer and de number of layers affects the ability of the MLP regressor to predict the real data. For training it tries to minimize the MSE (Mean Squared Error). The R2 is the R² and the MAE represents the Mean Absolute Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "country = 'United States'\n",
    "\n",
    "nn  = nn_by_country_train(country)\n",
    "nn_10_1 = nn_by_country_train(country,hidden_layer_sizes=(10))\n",
    "nn_10_2 = nn_by_country_train(country,hidden_layer_sizes=(10,10))\n",
    "nn_10_3 = nn_by_country_train(country,hidden_layer_sizes=(10,10,10))\n",
    "nn_10_4 = nn_by_country_train(country,hidden_layer_sizes=(10,10,10,10,10,10,10,10))\n",
    "nn_100_1 = nn_by_country_train(country,hidden_layer_sizes=(100))\n",
    "nn_100_2 = nn_by_country_train(country,hidden_layer_sizes=(100,100))\n",
    "nn_100_3 = nn_by_country_train(country,hidden_layer_sizes=(100,100,100))\n",
    "nn_100_4 = nn_by_country_train(country,hidden_layer_sizes=(100,100,100,100,100,100,100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The offset represents the number of days after the most recent data inserted in the dataset. Here we can conclude some things:\n",
    "\n",
    "1. The number of the hidden layers influences the type of regression the MLP uses, the more layers the more he can follow the curve as in small number of layers can only regress straight lines and >2 layers can regress curves\n",
    "2. After a certain amount of time, deppending on the country, the regression always tends to a linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print_predict(nn,country,offset=120,print_real=True,title=\"MLPRegressor Prediction\")\n",
    "print_predict(nn_10_1,country,offset=120,print_real=False, title=\"MLPRegressor Prediction (10)\")\n",
    "print_predict(nn_10_2,country,offset=120,print_real=False, title=\"MLPRegressor Prediction (10,10)\")\n",
    "print_predict(nn_10_3,country,offset=120,print_real=False, title=\"MLPRegressor Prediction (10,10,10)\")\n",
    "print_predict(nn_10_4,country,offset=120,print_real=False, title=\"MLPRegressor Prediction (10,10,10,10,10,10,10,10)\")\n",
    "print_predict(nn_100_1,country,offset=120,print_real=False,title=\"MLPRegressor Prediction (100)\")\n",
    "print_predict(nn_100_2,country,offset=120,print_real=False,title=\"MLPRegressor Prediction (100,100))\")\n",
    "print_predict(nn_100_3,country,offset=120,print_real=False,title=\"MLPRegressor Prediction (100,100,100)\")\n",
    "print_predict(nn_100_4,country,offset=120,print_real=False,title=\"MLPRegressor Prediction (100,100,100,100,100,100,100)\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbours\n",
    "\n",
    "In this chapter, the implementation of the K-Nearest Neighbours algorithm to solve the regression problem is demonstrated. To make this demonstration more understandable, it is divided in several functions with its explanation above.\n",
    "\n",
    "### Parameterization and application of KNN regression algorithm\n",
    "\n",
    "Based on the data set to be analysed, it is necessary to define the set of parameters that better adjusts to the current problem, in order to augment the efficiency and accuracy of the analysis.\n",
    "This is achieved by making a grid search, which, through the class' fit function, runs the algorithm several times trying to find the best combination of algorithms, as well as the best score, training the set in the process.\n",
    "\n",
    "Note that we use Pipeline, this is because GridSearchCV's fit method can receive multiple inputs (X_train) but can only receive a single output (Y_train), that is, can only receive one column, whilst we desire three: 'Confirmed', 'Deaths' and 'Recovered'; using a Pipeline with a StandardScaler and a MultiOutputRegressor allows working with more than one column in the output.\n",
    "\n",
    "In the following code block, a parameter grid is defined with the parameters and each one's possible values. These correspond to the KNeighborsRegressor's parameters. KNeighborsRegressor is a sklearn class that a adapts a regression model based on k-nearest neighbors.\n",
    "\n",
    "In order to avoid overfitting we use K-Folds cross-validation.\n",
    "\n",
    "Once fitting is concluded, we get the best estimator and print its best score and best parameter combination. This estimator is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_by_country_train(\n",
    "    country,\n",
    "    input=['Days Since Jan','Population Density (P/Km²)','Urban Population %','Pop %'],\n",
    "    out= ['Confirmed per million','Deaths per million','Recovered per million'],\n",
    "):\n",
    "    df = per_million_nolat.loc[ per_million_nolat['Country/Region'] == country]\n",
    "    X = df[input].values\n",
    "    Y = df[out].values\n",
    "\n",
    "    pipe_svr = Pipeline(\n",
    "                [\n",
    "                    (\"scl\", StandardScaler()),\n",
    "                    (\"knn\", MultiOutputRegressor(KNeighborsRegressor()))\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    parameter_grid = {'knn__estimator__n_neighbors': [3,5,7,9,11],\n",
    "                    'knn__estimator__weights': ['uniform','distance'],\n",
    "                    'knn__estimator__p' : [1,2,3],\n",
    "                    'knn__estimator__leaf_size' : [20,25,30,35,40,300]\n",
    "                    }\n",
    "\n",
    "    cross_validation = KFold(n_splits=10)\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=pipe_svr,\n",
    "                            param_grid=parameter_grid,\n",
    "                            cv=cross_validation,\n",
    "                            n_jobs=-1,\n",
    "                            return_train_score=True\n",
    "                            )\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.30,shuffle=True)\n",
    "\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "    knn = grid_search.best_estimator_\n",
    "\n",
    "    Y_pred = knn.predict(X_test)\n",
    "\n",
    "    score = knn.score(X_test,Y_test) # Calculate R² for the model\n",
    "    mae = mean_absolute_error(Y_test,Y_pred) # Mean Absolute Error\n",
    "    mse = mean_squared_error(Y_test,Y_pred) # Mean Squared Error\n",
    "\n",
    "    print(\"R2:\",score)\n",
    "    print(\"MAE:\",mae)\n",
    "    print(\"MSE:\",mse)\n",
    "\n",
    "    return knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Calling the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'Mexico'\n",
    "knn = knn_by_country_train(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_predict(knn,country,offset=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM - Support Vector Machine\n",
    "\n",
    "For predicting the number of cases of Covid-19, recovered patients, and deaths, we can develop a Support Vector Regression. This SVR would take in as input the Date, and output the mentioned parameters - Number of cases, recovered patients, and deaths. This data would refer to a specific country.\n",
    "In order to find the best parameters for the Regression at hand, we performed a grid search with a wide posssibility of values for different parameters (C, degree, epsilon, alpha). We also experimented with different kernels for the Regressor, and found different results for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(\n",
    "    country,\n",
    "    kernel,\n",
    "    input=['Days Since Jan','Population Density (P/Km²)','Urban Population %','Pop %'],\n",
    "    out= ['Confirmed per million','Deaths per million','Recovered per million'],\n",
    "    ):\n",
    "\n",
    "    df = per_million_nolat.loc[ per_million_nolat['Country/Region'] == country]\n",
    "    X = df[input].values\n",
    "    Y = df[out].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.30,shuffle=True)\n",
    "\n",
    "    pipe_svr = Pipeline(\n",
    "        [\n",
    "            (\"scl\", StandardScaler()),\n",
    "            (\"reg\", MultiOutputRegressor(SVR(kernel=kernel)))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    grid_param_svr = {\n",
    "    \"reg__estimator__C\": [0.1, 1, 10, 100, 1000, 10000],\n",
    "    \"reg__estimator__degree\": [2,3,4,5,6,7],\n",
    "    \"reg__estimator__epsilon\": [0.1, 0.01, 0.001, 0.0001, 0.00001],\n",
    "    \"reg__estimator__gamma\": [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1],\n",
    "    \"reg__estimator__coef0\": [1e-6, 1e-4, 1e-2, 1e-1, 0]\n",
    "    }\n",
    "\n",
    "    gs_svr = (\n",
    "        GridSearchCV(\n",
    "            estimator=pipe_svr,\n",
    "            param_grid=grid_param_svr,\n",
    "            cv=2,\n",
    "            scoring = \"neg_mean_squared_error\",\n",
    "            n_jobs = -1\n",
    "            )\n",
    "        )\n",
    "\n",
    "    gs_svr = gs_svr.fit(X_train, y_train)\n",
    "    gs_svr = gs_svr.best_estimator_\n",
    "\n",
    "    y_pred = gs_svr.predict(X_test)\n",
    "\n",
    "    r2 = gs_svr.score(X_test,y_test) # Calculate R² for the model\n",
    "    mae = mean_absolute_error(y_test,y_pred) # Mean Absolute Error\n",
    "    mse = mean_squared_error(y_test,y_pred) # Mean Squared Error\n",
    "\n",
    "    print(\"R2:\", r2)\n",
    "    print(\"MAE:\", mae)\n",
    "    print(\"MSE:\", mse)\n",
    "\n",
    "    return gs_svr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prediction_instance(kernels, future_days, countries):\n",
    "    for kernel in kernels:\n",
    "        for country_svn in countries:\n",
    "            gs_svr = make_prediction(country_svn, 'rbf')\n",
    "            print_predict(gs_svr,country_svn,offset=future_days, title=\"SVR with \" + kernel + \" kernel for \" + str(future_days) + \"days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = [\"Portugal\", \"Sweden\", \"United States\", \"Singapore\", \"Spain\", \"Italy\"]\n",
    "kernels = ['poly', 'linear', 'rbf']\n",
    "future_days = 60\n",
    "\n",
    "# This function prints predictions using all mentioned kernels, for the countries above. For this report, we will only print the results from Mexico.\n",
    "# print_prediction_instance(kernels, future_days, countries)\n",
    "\n",
    "print_prediction_instance(kernels, future_days, [\"Mexico\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for the SVM\n",
    "\n",
    "- Exponential kernel\n",
    "\n",
    "This kernel seems to be a good fit for the problem at hand, and is reaching very respectful scores. Since the growth of reported cases was exponential in the early weeks of the pandemic, the Regression developed by this kernel follows the same exponential tendency, which accounts for the good scores it reaches. However, if the number of cases continuies to drop or even stabilize, as it has in the past few weeks in manyu countries, this kernel may loose its relevancy for Covid-19 predictions in these specific countries.\n",
    "\n",
    "- Lineal kernel\n",
    "\n",
    "The scores of the regression (for most countries) show that this kernel is not the most appropriate for the given problem. However, by analysing the graphs, we see that, if the number of Covid-19 cases is expected to have a linear growth in the coming times, which the most recent data seems to suggest for many countries, it may become a better solution and accomplish better scores. However, since the growth was exponential in the first few weeks of the pandemic, this kernel lacks in score. Nevertheless, we consider it to be a good fit for the problem since the number of reported cases has been generally dropping significantly in recent times.\n",
    "\n",
    "- RBF kernel\n",
    "\n",
    "This kernel seems to be the most indicated for Covid-19 cases predictions, at least as far as scores go. Reaching close to perfect scores in some countries, it seems to fit really well with the problem at hand. However, analysing the graphs, we see that it struggles when the number of reported cases drop. We think that this is justified by the shortage in input Data for the regressor. In the future, we could expand into new data sets and solve this issue, or even study deeply how to work around this issue, as well as improving the predictions for all regressions.\n",
    "\n",
    "\n",
    "Overall, we conclude that finding the most fitting kernel for the regression depends on which country we are making predictions for. If the number of reported cases, deaths and recovered patients is, overall, increasing exponentially, the polynomial kernel is the most fitting. If, however, they are growing at a more or less constant rate, the linear kernel might be the best fit. At last, if they have been decreasing, the rbf kernel will most probably be the best fit, since it reaches the best scores. However, the issue with this kernel described above must be taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = [\"Portugal\", \"Sweden\", \"Spain\"]\n",
    "\n",
    "models = dict()\n",
    "\n",
    "for country in countries:\n",
    "    models[country] = []\n",
    "    models[country].append(nn_by_country_train(country))\n",
    "    models[country].append(knn_by_country_train(country))\n",
    "    models[country].append(make_prediction(country, \"rbf\"))\n",
    "    models[country].append(make_prediction(country, \"linear\"))\n",
    "    models[country].append(make_prediction(country, \"poly\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portugal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[\"Portugal\"]\n",
    "print_predict(model[0], \"Portugal\", offset=50, title=\"MLPRegressor prediction\")\n",
    "print_predict(model[1], \"Portugal\", offset=50, title=\"KNN prediction\", print_real=False)\n",
    "print_predict(model[1], \"Portugal\", offset=50, title=\"SVR prediction (rbf kernel)\", print_real=False)\n",
    "print_predict(model[2], \"Portugal\", offset=50, title=\"SVR prediction (linear kernel)\", print_real=False)\n",
    "print_predict(model[2], \"Portugal\", offset=50, title=\"SVR prediction (poly kernel)\", print_real=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sweden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[\"Sweden\"]\n",
    "print_predict(model[0], \"Sweden\", offset=50, title=\"MLPRegressor prediction\")\n",
    "print_predict(model[1], \"Sweden\", offset=50, title=\"KNN prediction\", print_real=False)\n",
    "print_predict(model[1], \"Sweden\", offset=50, title=\"SVR prediction (rbf kernel)\", print_real=False)\n",
    "print_predict(model[2], \"Sweden\", offset=50, title=\"SVR prediction (linear kernel)\", print_real=False)\n",
    "print_predict(model[2], \"Sweden\", offset=50, title=\"SVR prediction (poly kernel)\", print_real=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[\"Spain\"]\n",
    "print_predict(model[0], \"Spain\", offset=50, title=\"MLPRegressor prediction\")\n",
    "print_predict(model[1], \"Spain\", offset=50, title=\"KNN prediction\", print_real=False)\n",
    "print_predict(model[1], \"Spain\", offset=50, title=\"SVR prediction (rbf kernel)\", print_real=False)\n",
    "print_predict(model[2], \"Spain\", offset=50, title=\"SVR prediction (linear kernel)\", print_real=False)\n",
    "print_predict(model[2], \"Spain\", offset=50, title=\"SVR prediction (poly kernel)\", print_real=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By analysing the results, we conclude that we achieved some very good results, as well as other that were quite poor. Comparing the developed models, KNN seems to be the weakest and least fitting of the three, since the results achieved were not accurate or plausible. However, both the MLPRegressor and the SVR reached results which we are quite happy with."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python36964bit8180ba9c7c894caf8049c2abf5c083ab",
   "language": "python",
   "display_name": "Python 3.6.9 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}